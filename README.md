####################################
For French:
####################################
#------ pre-trained word vectors ---------#

file vecs100-linear-frwiki.zip or vecs50-linear-frwiki.bz2

- Training on a dump of Wikipedia
  (frwiki-20140804-corpus.xml.bz2 downloaded [here](http://linguatools.org/tools/corpora/wikipedia-monolingual-corpora/)) 
- preprocessing:
    -`xml2txt.pl` to remove xml tags
    - sxpipe-light for tokenizing   ( `perl ~/installation/melt-2.0b7/sxpipe-melt/segmenteur.pl < frwiki_raw.txt > frwiki_tokenized.txt` )
    - 650,353,499 tokens (3.6 Gb in raw text)
- vecs100:  `./word2vec -train frwiki_tokenized.txt -output vecs50 -threads 2 -min-count 100 -cbow 0 -negative 10`
- We use the default window size (window=5), which corresponds to a maximum window size of 11 (5 words on each side of the target); default number of iterations (5).

You can also use other word embeddings, such as 
# embeddings fasttext
cc.fr.300.vec (1.2G)
See https://fasttext.cc/docs/en/crawl-vectors.html

#------ Semantic resources ---------#
WOLF : http://almanach.inria.fr/software_and_resources/downloads/wolf-1.0b4.xml.bz2


#------ Evaluation dataset ---------#
- lexical similarity : file rg65_french.txt
- sentiment analysis : any benchmark you find online (please cite the source in your report)


#####################
For English: 
#####################

#------ pre-trained word vectors ---------#
file vectors_datatxt_250_sg_w10_i5_c500_gensim_clean.tar.bz2 

- data: concatenation of multiple corpora
    - (generated by the script in the original word2vec package)
    - cf: https://github.com/imsky/word2vec/blob/master/demo-train-big-model-v1.sh

- `gensim.models.Word2Vec(size = 250, min_count = 500, window=8, sample=1e-3, workers = 8, sg=1, hs=0, negative = 10, iter=5)`

#------ Semantic lexicons ---------#
- PPDB : http://paraphrase.org/#/download (use ppdb lexical xl size)
- wordnet 

#------ Evaluation dataset ---------#
- lexical similarity : file ws353.txt
- sentiment analysis : file stanford_sentiment_analysis.tar.gz
  https://nlp.stanford.edu/sentiment/
