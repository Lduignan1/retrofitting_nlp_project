{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "import math\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.linear_model import Perceptron"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Movie reviews**\n",
    "\n",
    "**Reading word vector files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test Perceptron classifier on word2vec embeddings\n",
    "\n",
    "# get document representations\n",
    "# results to be compared with retrofitted vectors\n",
    "\n",
    "# code: faruqui et al. (2015)\n",
    "\n",
    "# filename: txt file\n",
    "def read_word_vecs(filename):\n",
    "    \"\"\" Read all the word vectors and normalize them \"\"\"\n",
    "    word_vectors = {}\n",
    "    with (gzip.open(filename, 'rt') if filename.endswith('.gz') else open(filename, 'r')) as file:    \n",
    "        for line in file:\n",
    "            \n",
    "            line = line.strip().lower()\n",
    "            word = line.split()[0]\n",
    "            word_vectors[word] = np.zeros(len(line.split())-1, dtype=float)\n",
    "            for index, vec_val in enumerate(line.split()[1:]):\n",
    "                word_vectors[word][index] = float(vec_val)\n",
    "            # normalize weight vector \n",
    "            # a normalized vector points in the same direction as the original \n",
    "            # but has length 1\n",
    "            word_vectors[word] /= math.sqrt((word_vectors[word]**2).sum() + 1e-6)\n",
    "    \n",
    "    #sys.stderr.write(\"Vectors read from: \"+filename+\" \\n\")\n",
    "    return word_vectors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Converting movie reviews to averaged vectors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract reviews from files, convert to review vectors to be stored in numpy matrix (nb review, 250)\n",
    "\n",
    "# word_vectors: dict; filename: txt file; vec_size: int\n",
    "def reviews_to_vecs(word_vectors, filename, vec_size):\n",
    "    \"\"\"extract review texts from a file and convert them to averaged word2vec embeddings\"\"\"\n",
    "    \n",
    "    review_vectors = []\n",
    "    Y = []\n",
    "    with open(filename, 'r', encoding='utf-8') as file:      \n",
    "        for line in file:\n",
    "            \n",
    "            # initialize empty review vec of given size\n",
    "            review_vec = np.zeros(vec_size, dtype=float)\n",
    "            \n",
    "            line = line.strip().lower().split()\n",
    "            \n",
    "            # normalized/tokenized moview review, gold label\n",
    "            review, y = line[1:], line[0]\n",
    "            Y.append(y)\n",
    "            \n",
    "            # normalized/tokenized moview review\n",
    "            #review = line.split()[1:]\n",
    "            \n",
    "            for word in review:\n",
    "                if word in word_vectors:\n",
    "                    review_vec += word_vectors[word]\n",
    "                    \n",
    "            # get average of word vectors by dividing sum by nb of words in review\n",
    "            review_vec /= len(review)\n",
    "            \n",
    "            review_vectors.append(review_vec)\n",
    "    \n",
    "    # convert to numpy arrays\n",
    "    X = np.array(review_vectors)\n",
    "    Y = np.array(Y)\n",
    "    \n",
    "    return X, Y     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0389759  -0.01411923  0.00649187 ...  0.02806024  0.00548851\n",
      "   0.03144906]\n",
      " [ 0.06330878  0.01469497  0.01734737 ...  0.02479514  0.02445976\n",
      "   0.03876584]\n",
      " [ 0.05804808  0.00525672  0.01786497 ...  0.03130196  0.01622859\n",
      "   0.04347507]\n",
      " [ 0.04500403  0.02146169  0.01359255 ...  0.04601991  0.04227773\n",
      "   0.07036946]\n",
      " [ 0.04966516  0.00886954  0.01860892 ...  0.02242931  0.01846928\n",
      "   0.05119996]]\n",
      "Shape: (6920, 250)\n",
      "\n",
      "['1' '1' '1' '1' '1']\n",
      "Shape: (6920,)\n"
     ]
    }
   ],
   "source": [
    "wvPath = \"embeddings/vectors_datatxt_250_sg_w10_i5_c500_gensim_clean.gz\"\n",
    "word_vectors = read_word_vecs(wvPath)\n",
    "\n",
    "# getting train data\n",
    "SRTrainPath = \"datasets/stanford_sentiment_analysis/stanford_raw_train.txt\"\n",
    "X_train, Y_train = reviews_to_vecs(word_vectors, SRTrainPath, vec_size=250)\n",
    "\n",
    "print(X_train[:5])\n",
    "print(f\"Shape: {X_train.shape}\\n\")\n",
    "\n",
    "print(Y_train[:5])\n",
    "print(f\"Shape: {Y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.08391141  0.00493503  0.0007743  ...  0.02950124  0.00507889\n",
      "   0.06420888]\n",
      " [ 0.05613793  0.02151759  0.00791124 ...  0.02658185  0.00703825\n",
      "   0.05526056]\n",
      " [ 0.06949902  0.05778605 -0.00777598 ...  0.01285676  0.0061893\n",
      "   0.06091437]\n",
      " [ 0.05763503 -0.00330982  0.00640642 ...  0.03454663  0.02482536\n",
      "   0.07949061]\n",
      " [ 0.07521692  0.01980447  0.01480977 ...  0.01391576  0.02573809\n",
      "   0.06107445]]\n",
      "Shape: (1821, 250)\n",
      "\n",
      "['1' '1' '1' '1' '1']\n",
      "Shape: (1821,)\n"
     ]
    }
   ],
   "source": [
    "# getting test data\n",
    "SRTestPath = \"datasets/stanford_sentiment_analysis/stanford_raw_test.txt\"\n",
    "X_test, Y_test = reviews_to_vecs(word_vectors, SRTestPath, vec_size=250)\n",
    "\n",
    "print(X_test[:5])\n",
    "print(f\"Shape: {X_test.shape}\\n\")\n",
    "\n",
    "print(Y_test[:5])\n",
    "print(f\"Shape: {Y_test.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training a sklearn Perceptron**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron accuracy (original embeddings):  76.94%\n"
     ]
    }
   ],
   "source": [
    "# using pretrained vectors\n",
    "clf = Perceptron()\n",
    "\n",
    "clf = Perceptron(tol=1e-3, random_state=0)\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "print(f\"Perceptron accuracy (original embeddings): {clf.score(X_test, Y_test): .2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron accuracy (retrofitted embeddings):  50.08%\n"
     ]
    }
   ],
   "source": [
    "# using retrofitted vectors\n",
    "retrofitted_word_vectors = read_word_vecs(\"embeddings/out_faruqui_250.txt\")\n",
    "X_train_retrofit, Y_train_retrofit = reviews_to_vecs(retrofitted_word_vectors, SRTrainPath, vec_size=250)\n",
    "X_test_retrofit, Y_test_retrofit = reviews_to_vecs(retrofitted_word_vectors, SRTestPath, vec_size=250)\n",
    "\n",
    "clf = Perceptron()\n",
    "\n",
    "clf = Perceptron(tol=1e-3, random_state=0)\n",
    "clf.fit(X_train_retrofit, Y_train_retrofit)\n",
    "\n",
    "print(f\"Perceptron accuracy (retrofitted embeddings): {clf.score(X_test_retrofit, Y_test_retrofit): .2%}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarks**\n",
    "\n",
    "We see a ~2% increase in accuracy compared to Faruqui et al.'s best results\n",
    "\n",
    "These word vectors were retrofitted using the PPDB lexicon which seems to improve results more than others in this task\n",
    "\n",
    "We should see similar results with our own retrofitted vectors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using our retrofitted vectors from modified.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron accuracy (retrofitted embeddings):  79.68%\n"
     ]
    }
   ],
   "source": [
    "# using our retrofitted vectors\n",
    "retrofitted_word_vectors = read_word_vecs(\"embeddings/out_naz_250.txt\")\n",
    "X_train_retrofit, Y_train_retrofit = reviews_to_vecs(retrofitted_word_vectors, SRTrainPath, vec_size=250)\n",
    "X_test_retrofit, Y_test_retrofit = reviews_to_vecs(retrofitted_word_vectors, SRTestPath, vec_size=250)\n",
    "\n",
    "clf = Perceptron()\n",
    "\n",
    "clf = Perceptron(tol=1e-3, random_state=0)\n",
    "clf.fit(X_train_retrofit, Y_train_retrofit)\n",
    "\n",
    "print(f\"Perceptron accuracy (retrofitted embeddings): {clf.score(X_test_retrofit, Y_test_retrofit): .2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retrofitting_nlp_project-QymJoUec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
