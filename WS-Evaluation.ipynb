{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "499f8b04",
   "metadata": {},
   "source": [
    "# Evaluating embeddings via the semantic similarity task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "af2bb9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries \n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd5f5cf",
   "metadata": {},
   "source": [
    "## Reading files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9db13f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_embeddings(filename):\n",
    "    \"\"\"read a file containing word vectors and return their normalized forms in a dict\"\"\"\n",
    "    word_vecs = {}  # keys: str(words) ; values: np.array(normalized vectors)\n",
    "    with (gzip.open(filename, 'rt', encoding='utf-8') if filename.endswith('.gz') else open(filename, 'r', encoding='utf-8')) as file:  # 'rt' = open for reading as text file\n",
    "        for line in file:\n",
    "            elements = line.strip().lower().split()\n",
    "            word = elements[0]\n",
    "            vec = np.array([float(val) for val in elements[1:]], dtype=float)\n",
    "            \n",
    "            # get Euclidean norm\n",
    "            vec_norm = np.linalg.norm(vec)\n",
    "            \n",
    "            # normalize vector\n",
    "            word_vecs[word] = vec / vec_norm\n",
    "\n",
    "    return word_vecs\n",
    "\n",
    "word_vecs_pretrain = read_embeddings(\"embeddings/vectors_datatxt_250_sg_w10_i5_c500_gensim_clean\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "959dbdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vecs_retro = read_embeddings(\"embeddings/out_naz_ppdb_250.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "79a6e7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similiarity between pred embedding and `queen` embedding:  0.85\n"
     ]
    }
   ],
   "source": [
    "q_hat_king = word_vecs_pretrain[\"king\"]\n",
    "q_hat_man = word_vecs_pretrain[\"man\"]\n",
    "q_hat_woman = word_vecs_pretrain[\"woman\"]\n",
    "\n",
    "\n",
    "q_pred = q_hat_king - q_hat_man + q_hat_woman\n",
    "\n",
    "q_hat_queen = word_vecs_pretrain[\"queen\"]\n",
    "\n",
    "print(f\"similiarity between pred embedding and `queen` embedding: {np.dot(q_pred, q_hat_queen): .2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d7e82d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similiarity between pred embedding and `queen` retrofitted embedding:  0.85\n"
     ]
    }
   ],
   "source": [
    "q_king = word_vecs_retro[\"king\"]\n",
    "q_man = word_vecs_retro[\"man\"]\n",
    "q_woman = word_vecs_retro[\"woman\"]\n",
    "\n",
    "\n",
    "q_pred = q_king - q_man + q_woman\n",
    "\n",
    "q_queen = word_vecs_retro[\"queen\"]\n",
    "\n",
    "print(f\"similiarity between pred embedding and `queen` retrofitted embedding: {np.dot(q_pred, q_queen): .2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b546b103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity between `costly` and `pricey` before retrofitting:  0.44\n"
     ]
    }
   ],
   "source": [
    "q_hat_costly = word_vecs_pretrain[\"costly\"]\n",
    "\n",
    "q_hat_pricey = word_vecs_pretrain[\"pricey\"]\n",
    "\n",
    "print(f\"similarity between `costly` and `pricey` before retrofitting: {np.dot(q_hat_costly, q_hat_pricey): .2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "758aa9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity between `costly` and `pricey` after retrofitting:  0.78\n"
     ]
    }
   ],
   "source": [
    "q_costly = word_vecs_retro[\"costly\"]\n",
    "\n",
    "q_pricey = word_vecs_retro[\"pricey\"]\n",
    "\n",
    "print(f\"similarity between `costly` and `pricey` after retrofitting: {np.dot(q_costly, q_pricey): .2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f379cc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_Y(filename, word_vecs):\n",
    "    \"\"\"read a word similarity file and return arrays containing human scores and system scores respectively\"\"\"\n",
    "    with open(filename, \"r\", encoding='utf-8') as file:\n",
    "        \n",
    "        X = [] # human scores\n",
    "        Y = [] # system scores\n",
    "        for line in file:\n",
    "            \n",
    "            line = line.strip().split()\n",
    "     \n",
    "            # skip words we don't have embeddings for\n",
    "            if line[0] not in word_vecs or line[1] not in word_vecs:\n",
    "                continue\n",
    "                \n",
    "            # human score is last element in line\n",
    "            X.append(float(line[-1]))\n",
    "                        \n",
    "            # get dot product since vectors already normalized\n",
    "            Y.append(np.dot(word_vecs[line[0]], word_vecs[line[1]]))\n",
    "            \n",
    "    return np.array(X), np.array(Y)\n",
    "    \n",
    "X, Y_pretrain = get_X_Y(\"datasets/ws353.txt\", word_vecs_pretrain)\n",
    "X, Y_retro = get_X_Y(\"datasets/ws353.txt\", word_vecs_retro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3dffceea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(335,)\n",
      "(335,)\n"
     ]
    }
   ],
   "source": [
    "print(Y_retro.shape)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ac98ad",
   "metadata": {},
   "source": [
    "## Using `scipy.stats.spearmanr` to compute and compare word similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3a4945b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman's correlation (original vectors):  0.69\n",
      "Spearman's correlation (retrofitted vectors):  0.64\n"
     ]
    }
   ],
   "source": [
    "# computing Spearman correlation coefficient for pretrained vectors and retrofitted ppdb vectors\n",
    "res_pretrain = scipy.stats.spearmanr(X, Y_pretrain)\n",
    "\n",
    "res_retro = scipy.stats.spearmanr(X, Y_retro)\n",
    "\n",
    "print(f\"Spearman's correlation (original vectors): {res_pretrain[0]: .2}\")\n",
    "print(f\"Spearman's correlation (retrofitted vectors): {res_retro[0]: .2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842b1147",
   "metadata": {},
   "source": [
    "## Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1ef2ae",
   "metadata": {},
   "source": [
    "- Slight decrase in Spearman correlation when using ppdb but original pretrained vectors already correlate relatively highly\n",
    "- Lower as with vectors retrofitted using Faruqui et al.'s original program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3702bc29",
   "metadata": {},
   "source": [
    "## French data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "553a336e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
