{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00fb4ec5",
   "metadata": {},
   "source": [
    "# Evaluating embeddings via the semantic similarity task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6af55737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries \n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3ff508",
   "metadata": {},
   "source": [
    "## Reading files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a90fa319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_embeddings(filename):\n",
    "    \"\"\"read a file containing word vectors and return their normalized forms in a dict\"\"\"\n",
    "    word_vecs = {}  # keys: str(words) ; values: np.array(normalized vectors)\n",
    "    with (gzip.open(filename, 'rt', encoding='utf-8') if filename.endswith('.gz') else open(filename, 'r', encoding='utf-8')) as file:  # 'rt' = open for reading as text file\n",
    "        for line in file:\n",
    "            elements = line.strip().lower().split()\n",
    "            word = elements[0]\n",
    "            vec = np.array([float(val) for val in elements[1:]], dtype=float)\n",
    "            \n",
    "            # get Euclidean norm\n",
    "            vec_norm = np.linalg.norm(vec)\n",
    "            \n",
    "            # normalize vector\n",
    "            word_vecs[word] = vec / vec_norm\n",
    "\n",
    "    return word_vecs\n",
    "\n",
    "word_vecs_pretrain = read_embeddings(\"embeddings/vectors_datatxt_250_sg_w10_i5_c500_gensim_clean\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7bb4fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vecs_retro = read_embeddings(\"embeddings/out_naz_250.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c4c8b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_Y(filename, word_vecs):\n",
    "    \"\"\"read a word similarity file and return arrays containing human scores and system scores respectively\"\"\"\n",
    "    with open(filename, \"r\", encoding='utf-8') as file:\n",
    "        \n",
    "        X = [] # human scores\n",
    "        Y = [] # system scores\n",
    "        for line in file:\n",
    "            \n",
    "            line = line.strip().split()\n",
    "     \n",
    "            # skip words we don't have embeddings for\n",
    "            if line[0] not in word_vecs or line[1] not in word_vecs:\n",
    "                continue\n",
    "                \n",
    "            # human score is last element in line\n",
    "            X.append(float(line[-1]))\n",
    "                        \n",
    "            # get dot product since vectors already normalized\n",
    "            Y.append(np.dot(word_vecs[line[0]], word_vecs[line[1]]))\n",
    "            \n",
    "    return np.array(X), np.array(Y)\n",
    "    \n",
    "X, Y_pretrain = get_X_Y(\"datasets/ws353.txt\", word_vecs_pretrain)\n",
    "X, Y_retro = get_X_Y(\"datasets/ws353.txt\", word_vecs_retro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ad31337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(335,)\n",
      "(335,)\n"
     ]
    }
   ],
   "source": [
    "print(Y_retro.shape)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ab62b9",
   "metadata": {},
   "source": [
    "## Using `scipy.stats.spearmanr` to compute and compare word similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d62ffe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman's correlation (original vectors):  0.69\n",
      "Spearman's correlation (retrofitted vectors):  0.71\n"
     ]
    }
   ],
   "source": [
    "# computing Spearman correlation coefficient for pretrained vectors and retrofitted vectors\n",
    "res_pretrain = scipy.stats.spearmanr(X, Y_pretrain)\n",
    "\n",
    "res_retro = scipy.stats.spearmanr(X, Y_retro)\n",
    "\n",
    "print(f\"Spearman's correlation (original vectors): {res_pretrain[0]: .2}\")\n",
    "print(f\"Spearman's correlation (retrofitted vectors): {res_retro[0]: .2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d10888",
   "metadata": {},
   "source": [
    "## Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff7194a",
   "metadata": {},
   "source": [
    "- Slight increase in Spearman correlation but original pretrained vectors already correlate relatively highly\n",
    "- Same result as with vectors retrofitted using Faruqui et al.'s original program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb79ad4",
   "metadata": {},
   "source": [
    "## French data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ec19907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
