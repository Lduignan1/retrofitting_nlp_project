19 May. Nazanin, Liam: be able to explain what the project is about.
23 May. Nazanin: make a "skeleton" of the program, to_do steps, divide coding tasks.
        Liam: Theoretical background.
        
28 May. Nazanin: finish implementing functions that allow our retrofit program to run
        Liam: Research theoretical background on distributional semantics, word2vec, semantic lexicons; evaluation metrics
        
29 May. Liam: figure out what's causing program to generate a few erratic values; 
        Liam: word similarity metric

30. May. Tasks left to do:
        - implement word similarity metric (Liam) - done
        - finish writing theoretical part (Liam) - done
        - slides for theoretical part (Liam)
        - write implementation part (Nazanin) - Done
        - slides implementation part (Nazanin)
        - write about experiments/results (Liam) - In progress
        - slides experiments/results (Liam)
        - write user manual/README (Nazanin)
        - extracting data from WN using NLTK (Nazanin) - Done
        - extracting data from PPDB - done
        
        
1 June. Update
        - General algorithm and eval metrics mostly finished
        - make sure our code is as original-looking as possible; no re-used comments or variable names (Nazanin) - Done?
        - need to figure out how to manually extract data from semantic lexicons - do we keep or remove read_lexicon funct?
        - see new outline and suggestions from Bingzhi
        - perhaps a little script to run and output scores on either SA or WS (or maybe just the notebooks are fine)
        - would be nice to retrofit some french embeddings i think with FreNetic (not a priority)
        
5 June. Update
        - ppdb extraction implemented but slightly lower word similarity score vs original embeddings; i think this is because we can only download ppdb 2.0 whereas the one in faruqui's original repo is ppdb 1.0
        - take a look at the read_ppdb and decide how you want to deal with lexicon choice at runtime
        - i'm not sure if its worth writing a script to show the evaluation scores; maybe just the notebooks are fine
        - still need to look at french data; i will do that tomorrow
        
7 June. Update
        - update implementation part (Nazanin) - Done
        - write user manual/README (Nazanin) - In progress
        - write about experiments/results (Liam) - In progress
        - retrofit some french embeddings (Liam) - Done
        - review code and report for consistency and finalize project (Nazanin, Liam)
        
        ** to be done after report + code handed in
        - slides for theoretical part (Liam)
        - slides implementation part (Nazanin)
        - slides experiments/results (Liam)
        
June 8. 
I just saw in the notes I had taken from the first meeting:
- test different values for alpha and beta
- Stopping criteria: when Euclidian distance is smaller than 0.01 (criteria of the paper) We need to define our own stopping criteria
(What are your thoughts on this? Should we do it ir just leave it?)

10 June Update
        - French embeddings successfully retrofitted with French ppdb 1.0 and wordnet
        - new 'lang' argument when executing program; 'eng' or 'fra'
        - removed check if embeddings file ends in .txt or .gz; french embeddings come in a generic file with no extension
        - got rid of all parts of retrofit function that make words lowercase; many French words in embeddings file are uppercase and this was resulting in key                 errors
        - results for English and French in Google doc and notebooks
        - was thinking about testing diff values for alpha, beta and num_iter but the original authors don't; claim best results when alpha = 1, beta =                         1/num_neighbors and num_iter = 10 but i will see if i have time
          
          (Nazanin)
        - modified the normalize function in lexicon.py to also be able to capture French letters. 
          But in order to check it myself I would need to dl a lot of French datasets including the French movie reviews. 
          So if it's alright, I'll leave it to you to check if performance increases.
          Maybe this could also be related to why you were getting KeyErrors on the French datasets?
          
11 June Update
        - fininshed updating the Implementation section of the report (hopefully for the last time)
        - updated lexicon.py to make the computations more efficient
