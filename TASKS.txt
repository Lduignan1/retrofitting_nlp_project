19 May. Nazanin, Liam: be able to explain what the project is about.
23 May. Nazanin: make a "skeleton" of the program, to_do steps, divide coding tasks.
        Liam: Theoretical background.
        
28 May. Nazanin: finish implementing functions that allow our retrofit program to run
        Liam: Research theoretical background on distributional semantics, word2vec, semantic lexicons; evaluation metrics
        
29 May. Liam: figure out what's causing program to generate a few erratic values; 
        Liam: word similarity metric

30. May. Tasks left to do:
        - implement word similarity metric (Liam) - done
        - finish writing theoretical part (Liam) - done
        - slides for theoretical part (Liam)
        - write implementation part (Nazanin) - Done
        - slides implementation part (Nazanin)
        - write about experiments/results (Liam) - In progress
        - slides experiments/results (Liam)
        - write user manual/README (Nazanin)
        - extracting data from WN using NLTK (Nazanin) - Done
        - extracting data from PPDB - done
        
        
1 June. Update
        - General algorithm and eval metrics mostly finished
        - make sure our code is as original-looking as possible; no re-used comments or variable names (Nazanin) - Done?
        - need to figure out how to manually extract data from semantic lexicons - do we keep or remove read_lexicon funct?
        - see new outline and suggestions from Bingzhi
        - perhaps a little script to run and output scores on either SA or WS (or maybe just the notebooks are fine)
        - would be nice to retrofit some french embeddings i think with FreNetic (not a priority)
        
5 June. Update
        - ppdb extraction implemented but slightly lower word similarity score vs original embeddings; i think this is because we can only download ppdb 2.0 whereas the one in faruqui's original repo is ppdb 1.0
        - take a look at the read_ppdb and decide how you want to deal with lexicon choice at runtime
        - i'm not sure if its worth writing a script to show the evaluation scores; maybe just the notebooks are fine
        - still need to look at french data; i will do that tomorrow
        
7 June. Update
        - update implementation part (Nazanin) - In progress
        - write user manual/README (Nazanin)
        - write about experiments/results (Liam) - In progress
        - retrofit some french embeddings (Liam) - Done
        - review code and report for consistency and finalize project (Nazanin, Liam)
        
        ** to be done after report + code handed in
        - slides for theoretical part (Liam)
        - slides implementation part (Nazanin)
        - slides experiments/results (Liam)
        
June 8. 
I just saw in the notes I had taken from the first meeting:
- test different values for alpha and beta
- Stopping criteria: when Euclidian distance is smaller than 0.01 (criteria of the paper) We need to define our own stopping criteria
(What are your thoughts on this? Should we do it ir just leave it?)
